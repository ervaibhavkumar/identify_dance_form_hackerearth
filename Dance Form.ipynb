{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport cv2\nimport math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom keras.utils.np_utils import to_categorical\nfrom skimage.transform import rotate","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Get files from a path"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_files(path):\n    files = list()\n    \n    # /kaggle/input/dataset/train\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            files.append(os.path.join(dirname, filename))\n    \n    return files","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files_path = get_files('/kaggle/input/dance-form-vaibhav/dataset/train/')\n\ntrain_csv = pd.read_csv('/kaggle/input/dance-form-vaibhav/dataset/train.csv')\n\nprint(\"Train files : {0}\".format(len(train_files_path)))","execution_count":3,"outputs":[{"output_type":"stream","text":"Train files : 364\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dict = dict(zip(train_csv.Image, train_csv.target))\nprint(train_dict['96.jpg'])","execution_count":4,"outputs":[{"output_type":"stream","text":"manipuri\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Split into training and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(len(train_files_path) * 0.83)\nrandom.shuffle(train_files_path)\ntrain_files = train_files_path[:split]\nval_files = train_files_path[split:]\n\nprint(\"Training files {0}\".format(len(train_files)))\nprint(\"Validation files {0}\".format(len(val_files)))","execution_count":5,"outputs":[{"output_type":"stream","text":"Training files 302\nValidation files 62\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Converting dance forms into numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_labels = list(set(train_dict.values()))\nunique_labels_mapping = dict()\nfor index in range(len(unique_labels)):\n    unique_labels_mapping[unique_labels[index]] = index\n\ntotal_classes = len(unique_labels)\nprint(\"Number of classes : {0}\".format(total_classes))","execution_count":6,"outputs":[{"output_type":"stream","text":"Number of classes : 8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_one_hot_encoded_mask(value, num_labels):\n    return to_categorical(value, num_classes = num_labels)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting up hyper-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width, img_height = 256, 256\nbatch_size = 16\nepochs = 10\nlearning_rate = 1e-3","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rotate_image(image, angle_list):\n    rotated_images = list()\n    for angle in angle_list:\n        rotated_images.append(rotate(image,angle))\n    \n    return rotated_images","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaleDown_image(image, fx=0.6, fy=0.6):\n    return cv2.resize(image, None, fx= 0.6, fy= 0.6, interpolation= cv2.INTER_LINEAR)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaleUp_image(image, fx = 2, fy = 2):\n    return cv2.resize(image, None, fx = fx, fy = fy, interpolation= cv2.INTER_LINEAR)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def horizontal_flip(img):\n    return img[: , ::-1]","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating DataGenerator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator:\n    def __init__(self, train_files, valid_files, labels_dict, batch_size = 16, val_augment = True):\n        self.train_files = train_files\n        self.valid_files = valid_files\n        self.labels_dict = labels_dict\n        self.batch_size = batch_size\n        self.val_augment = val_augment\n        \n    def train_generator(self):\n        num_images = len(self.train_files)\n        while True:\n            x_batch = list()\n            y_batch = list()\n            index_list = list(range(0,num_images))\n            index_list = shuffle(index_list)\n            for idxs in range(0, num_images, self.batch_size):\n                x_batch = list()\n                y_batch = list()\n                for idx in index_list[idxs:min(idxs+self.batch_size, num_images)]:\n                    \n                    img = cv2.imread(self.train_files[idx])\n                    img = cv2.resize(img, (img_width, img_height))\n                    img = img / 255\n                    x_batch.append(img)\n                    \n                    image_name = self.train_files[idx].split(\"/\")\n                    label = unique_labels_mapping[train_dict[str(image_name[-1])]]\n                    label = get_one_hot_encoded_mask(label, total_classes)\n                    y_batch.append(label)\n                    \n                    rotated_images = rotate_image(img, [45, 60, -45, -60])\n                    for rotated_image in rotated_images:\n                        x_batch.append(rotated_image)\n                        y_batch.append(label)\n                                        \n                    x_batch.append(horizontal_flip(img))\n                    y_batch.append(label)\n                \n                x_batch, y_batch = shuffle(x_batch, y_batch, random_state=0)\n                yield (np.asarray(x_batch), np.asarray(y_batch))\n    \n    def valid_generator(self):\n        num_images = len(self.valid_files)\n        while True:\n            x_batch = list()\n            y_batch = list()\n            index_list = list(range(0,num_images))\n            index_list = shuffle(index_list)\n            for idxs in range(0, num_images, self.batch_size):\n                x_batch = list()\n                y_batch = list()\n                for idx in index_list[idxs:min(idxs+self.batch_size, num_images)]:\n                    \n                    img = cv2.imread(self.valid_files[idx])\n                    img = cv2.resize(img, (img_width, img_height))\n                    img  = img / 255\n                    x_batch.append(img)\n                    \n                    image_name = self.valid_files[idx].split(\"/\")\n                    label = unique_labels_mapping[train_dict[str(image_name[-1])]]\n                    label = get_one_hot_encoded_mask(label, total_classes)\n                    y_batch.append(label)\n                    \n                    if self.val_augment:\n                        rotated_images = rotate_image(img, [45, 60, -45, -60])\n                        for rotated_image in rotated_images:\n                            x_batch.append(rotated_image)\n                            y_batch.append(label)\n\n                        x_batch.append(horizontal_flip(img))\n                        y_batch.append(label)\n                        \n                x_batch, y_batch = shuffle(x_batch, y_batch, random_state=0)\n                yield (np.asarray(x_batch), np.asarray(y_batch))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch_steps = int(math.ceil(len(train_files) / batch_size) * 6)\nprint(epoch_steps)\nval_steps = int(math.ceil(len(val_files) / batch_size) * 6)\nprint(val_steps)","execution_count":14,"outputs":[{"output_type":"stream","text":"114\n24\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Model Experiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import vgg19, resnet50, inception_v3, inception_resnet_v2, mobilenet_v2\nfrom keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Model\nimport tensorflow.keras as keras","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VGG19 Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = vgg19.VGG19(include_top=False, input_shape=(img_width, img_height, 3), \\\n                    weights='/kaggle/input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = model.output\nx = Flatten()(x)\nx = Dense(512,activation='relu')(x)\nx = Dense(256,activation='relu')(x)\nfinal_layer = Dense(total_classes,activation='softmax')(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Model(inputs=model.input,outputs=final_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(learning_rate=learning_rate , momentum = 0.9), \n              loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = DataGenerator(train_files, val_files, train_dict, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=datagen.train_generator() , steps_per_epoch=epoch_steps, \n                             epochs=10, validation_steps = val_steps, \n                             validation_data=datagen.valid_generator(), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNet50 Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = resnet50.ResNet50(include_top=False, input_shape=(img_width, img_height, 3),\n                    weights='/kaggle/input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512,activation='relu')(x)\nfinal_layer = Dense(total_classes,activation='softmax')(x)\n\nmodel=Model(inputs=base_model.input,outputs=final_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(learning_rate=learning_rate , momentum = 0.9), \n              loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = DataGenerator(train_files, val_files, train_dict, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=datagen.train_generator() , steps_per_epoch=epoch_steps, \n                             epochs=20, validation_steps = val_steps, \n                             validation_data=datagen.valid_generator(), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### InceptionV3 Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = inception_v3.InceptionV3(include_top=False, input_shape=(img_width, img_height, 3),\n                    weights='/kaggle/input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512,activation='relu', kernel_regularizer='l1')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,activation='relu')(x)\nfinal_layer = Dense(total_classes,activation='softmax')(x)\n\nmodel=Model(inputs=base_model.input,outputs=final_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(learning_rate=learning_rate , momentum = 0.9), \n              loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = DataGenerator(train_files, val_files, train_dict, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_scheduler(epoch, lr):\n#     if epoch % 2 == 0:\n#         return lr\n    return lr * 1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n    keras.callbacks.LearningRateScheduler(lr_scheduler),\n#     keras.callbacks.ModelCheckpoint(filepath='/kaggle/working/models.{epoch:03d}.hdf5',\n#                     monitor='loss', verbose=2, save_best_only=False)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=datagen.train_generator() , steps_per_epoch=epoch_steps, \n                             epochs=20, validation_steps = val_steps, \n                             validation_data=datagen.valid_generator(), verbose=2, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### InceptionResnetV2 Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = inception_resnet_v2.InceptionResNetV2(include_top=False, input_shape=(img_width, img_height, 3),\n                    weights='/kaggle/input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512,activation='relu')(x)\nfinal_layer = Dense(total_classes,activation='softmax')(x)\n\nmodel=Model(inputs=base_model.input,outputs=final_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(learning_rate=learning_rate , momentum = 0.9), \n              loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = DataGenerator(train_files, val_files, train_dict, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=datagen.train_generator() , steps_per_epoch=epoch_steps, \n                             epochs=10, validation_steps = val_steps, \n                             validation_data=datagen.valid_generator(), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MobileNetV2 Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = mobilenet_v2.MobileNetV2(include_top=False, input_shape=(img_width, img_height, 3),\n                    weights='/kaggle/input/mobilenet-v2-128/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5')\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512,activation='relu')(x)\nfinal_layer = Dense(total_classes,activation='softmax')(x)\n\nmodel=Model(inputs=base_model.input,outputs=final_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(learning_rate=learning_rate , momentum = 0.9), \n              loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = DataGenerator(train_files, val_files, train_dict, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=datagen.train_generator() , steps_per_epoch=epoch_steps, \n                             epochs=10, validation_steps = val_steps, \n                             validation_data=datagen.valid_generator(), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DeXpression Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_FeatCNN(input_shape,classes):\n    padding = 'valid'\n    img_input = keras.layers.Input(shape=input_shape)\n\n    # START MODEL\n    conv_1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding=padding, activation='relu', name='conv_1')(img_input)\n    maxpool_1 = keras.layers.MaxPooling2D((2, 2), strides=(2,2))(conv_1)\n    x = tf.keras.layers.BatchNormalization()(maxpool_1)\n    \n    # FEAT-EX1\n    conv_2a = tf.keras.layers.Conv2D(96, (1, 1), strides=(1,1), activation='relu', padding=padding, name='conv_2a')(x)\n    conv_2b = tf.keras.layers.Conv2D(208, (3, 3), strides=(1,1), activation='relu', padding=padding, name='conv_2b')(conv_2a)\n    maxpool_2a = keras.layers.MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_2a')(x)\n    conv_2c = tf.keras.layers.Conv2D(64, (1, 1), strides=(1,1), name='conv_2c')(maxpool_2a)\n    concat_1 = tf.keras.layers.concatenate(inputs=[conv_2b,conv_2c], axis=3,name='concat2')\n    maxpool_2b = keras.layers.MaxPooling2D((3,3), strides=(2,2), padding=padding, name='maxpool_2b')(concat_1)\n\n    # FEAT-EX2\n    conv_3a = tf.keras.layers.Conv2D(96, (1, 1), strides=(1,1), activation='relu', padding=padding, name='conv_3a')(maxpool_2b)\n    conv_3b = tf.keras.layers.Conv2D(208, (3, 3), strides=(1,1), activation='relu', padding=padding, name='conv_3b')(conv_3a)\n    maxpool_3a = keras.layers.MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_3a')(maxpool_2b)\n    conv_3c = tf.keras.layers.Conv2D(64, (1, 1), strides=(1,1), name='conv_3c')(maxpool_3a)\n    concat_3 = tf.keras.layers.concatenate(inputs=[conv_3b,conv_3c],axis=3,name='concat3')\n    maxpool_3b = keras.layers.MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_3b')(concat_3)\n    \n    # FEAT-EX3\n#     conv_4a = tf.keras.layers.Conv2D(96, (1, 1), strides=(1,1), activation='relu', padding=padding, name='conv_4a')(maxpool_3b)\n#     conv_4b = tf.keras.layers.Conv2D(208, (3, 3), strides=(1,1), activation='relu', padding=padding, name='conv_4b')(conv_4a)\n#     maxpool_4a = keras.layers.MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_4a')(maxpool_3b)\n#     conv_4c = tf.keras.layers.Conv2D(64, (1, 1), strides=(1,1), name='conv_4c')(maxpool_4a)\n#     concat_4 = tf.keras.layers.concatenate(inputs=[conv_4b,conv_4c],axis=3,name='concat4')\n#     maxpool_4b = keras.layers.MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_4b')(concat_4)\n    \n    # FINAL LAYERS\n    net = tf.keras.layers.Flatten()(maxpool_3b)\n    \n    x = Dense(512,activation='relu')(net)\n    x = Dense(256,activation='relu')(x)\n    final_layer = Dense(classes,activation='softmax')(x)\n    \n    # Create model.\n    model = tf.keras.Model(img_input, final_layer, name='deXpression')\n    return model","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = my_FeatCNN((img_width,img_height,3), total_classes)\nmodel.summary()","execution_count":17,"outputs":[{"output_type":"stream","text":"Model: \"deXpression\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n__________________________________________________________________________________________________\nconv_1 (Conv2D)                 (None, 126, 126, 64) 4864        input_1[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 63, 63, 64)   0           conv_1[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 63, 63, 64)   256         max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv_2a (Conv2D)                (None, 63, 63, 96)   6240        batch_normalization[0][0]        \n__________________________________________________________________________________________________\nmaxpool_2a (MaxPooling2D)       (None, 61, 61, 64)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv_2b (Conv2D)                (None, 61, 61, 208)  179920      conv_2a[0][0]                    \n__________________________________________________________________________________________________\nconv_2c (Conv2D)                (None, 61, 61, 64)   4160        maxpool_2a[0][0]                 \n__________________________________________________________________________________________________\nconcat2 (Concatenate)           (None, 61, 61, 272)  0           conv_2b[0][0]                    \n                                                                 conv_2c[0][0]                    \n__________________________________________________________________________________________________\nmaxpool_2b (MaxPooling2D)       (None, 30, 30, 272)  0           concat2[0][0]                    \n__________________________________________________________________________________________________\nconv_3a (Conv2D)                (None, 30, 30, 96)   26208       maxpool_2b[0][0]                 \n__________________________________________________________________________________________________\nmaxpool_3a (MaxPooling2D)       (None, 28, 28, 272)  0           maxpool_2b[0][0]                 \n__________________________________________________________________________________________________\nconv_3b (Conv2D)                (None, 28, 28, 208)  179920      conv_3a[0][0]                    \n__________________________________________________________________________________________________\nconv_3c (Conv2D)                (None, 28, 28, 64)   17472       maxpool_3a[0][0]                 \n__________________________________________________________________________________________________\nconcat3 (Concatenate)           (None, 28, 28, 272)  0           conv_3b[0][0]                    \n                                                                 conv_3c[0][0]                    \n__________________________________________________________________________________________________\nmaxpool_3b (MaxPooling2D)       (None, 26, 26, 272)  0           concat3[0][0]                    \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 183872)       0           maxpool_3b[0][0]                 \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 512)          94142976    flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 8)            2056        dense_1[0][0]                    \n==================================================================================================\nTotal params: 94,695,400\nTrainable params: 94,695,272\nNon-trainable params: 128\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=SGD(learning_rate=0.001 , momentum = 0.9), \n              loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = DataGenerator(train_files, val_files, train_dict, batch_size)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_scheduler(epoch, lr):\n#     if epoch % 2 == 0:\n#         return lr\n    return lr * 1.0","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n    keras.callbacks.LearningRateScheduler(lr_scheduler),\n#     keras.callbacks.ModelCheckpoint(filepath='/kaggle/working/models.{epoch:03d}.hdf5',\n#                     monitor='loss', verbose=2, save_best_only=False)\n]","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(generator=datagen.train_generator() , steps_per_epoch=epoch_steps, \n                             epochs=20, validation_steps = val_steps, \n                             validation_data=datagen.valid_generator(), verbose=2, callbacks=callbacks)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n114/114 - 92s - loss: 1.8071 - accuracy: 0.3415 - val_loss: 2.0180 - val_accuracy: 0.2151 - lr: 0.0010\nEpoch 2/20\n114/114 - 94s - loss: 0.9423 - accuracy: 0.6720 - val_loss: 1.9147 - val_accuracy: 0.3244 - lr: 0.0010\nEpoch 3/20\n114/114 - 92s - loss: 0.3882 - accuracy: 0.8844 - val_loss: 1.4811 - val_accuracy: 0.5193 - lr: 0.0010\nEpoch 4/20\n114/114 - 93s - loss: 0.0949 - accuracy: 0.9846 - val_loss: 1.5305 - val_accuracy: 0.5475 - lr: 0.0010\nEpoch 5/20\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Generate Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_map = {v: k for k, v in unique_labels_mapping.items()}\nprint(inv_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = pd.read_csv('/kaggle/input/dance-form-vaibhav/dataset/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = test_csv['Image']\nprint(test_images[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = list()\nfor index in range(len(test_images)):\n    img = cv2.imread('/kaggle/input/dance-form-vaibhav/dataset/test/' + str(test_images[index]))\n    img = cv2.resize(img, (img_width, img_height))\n    img = img / 255\n    img = np.expand_dims(img, axis=0)\n    pred = model.predict(img)\n    output.append(inv_map[np.argmax(pred)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Image': test_images,\n                   'target': output})\ndf.to_csv(\"/kaggle/working/output.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}